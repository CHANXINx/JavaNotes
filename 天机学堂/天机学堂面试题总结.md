# 可扩展部分：
- 后台播放？
- 使用数据库持久化存储在Redis中的点赞记录；
- 使用Redisson替代DelayQueue实现延时任务；
- XXL-JOB替代SpringTask实现定时任务=》点赞记录
# 架构总览
## 1. 技术架构：
![[Pasted image 20241124202036.png]]
## 2. 系统架构：
![[Pasted image 20241123203307.png]]
## 3. 功能演示：

**老师核心业务：**![[Pasted image 20241123203430.png]]
**学生核心业务：**
![[Pasted image 20241123203454.png]]
## 4. 项目模块
![[Pasted image 20241123203545.png]]


# <font color="#245bdb">学习计划和进度系统</font>

>参与设计和开发了学习计划和学习进度统计功能。设计了一套**高性能**的视频播放进度记录系统，在不增加数据库压力的情况下，使视频播放进度回放精度达到**秒级**。

## 业务分析：-
用户可在购买课程后，可以对已购课程添加学习计划，并对应计算出预计完成时间。有了计划以后，就可以在课程页面展示用户计划的完成情况。

创建学习计划：
![[Pasted image 20241124150800.png|400]]
课程页面展示计划进度：![[Pasted image 20241124150850.png]]
这里引出重点，如何统计学习进度？如何判断某个课程是否已完成？

### 学习进度统计：
一个课程分为多个章节，每个章节都有对应视频以及章节测试（考试）。对于考试，已提交试卷就代表该课程已结束；而对于视频课程，如何统计是否已完成呢？
![[Pasted image 20241124151330.png]]

这里规定：视频播放进度超过50%，就代表这一课程已学完。
所以用户在播放视频的过程中：
1. 需要**不断提交学习进度**，当进度超过50%，则代表该课程已完成。
2. 需要记录用户播放进度（记录播放到**第几秒**），以便下一次打开能自动续播，增强用户体验。

因此，要记录用户学习进度，要保存以下信息：
1. 用户学习的是哪一个小节？（课程id，小节id）
2. 当前播放进度；
3. 小节是否已经学完？

具体流程如下：
![[Pasted image 20241124152734.png]]


## 1. 你在开发中参与了哪些功能开发让你觉得比较有挑战性？
我参与了整个学习中心的功能开发，其中有很多的学习辅助功能都很有特色。比如视频播放的进度记录。我们网站的课程是以录播视频为主，为了提高用户的学习体验，需要实现**视频续播**功能。这个功能本身并不复杂，只不过我们产品提出的要求比较高：
- 首先续播**时间误差要控制在30秒以内**。
- 而且要做到用户突然断开，甚至切换设备后，都可以继续上一次播放

要达成这个目的，使用传统的手段显然是不行的。
首先，要做到切换设备后还能续播，用户的播放进度必须保存在服务端，而不是客户端。
其次，用户突然断开或者切换设备，续播的时间误差不能超过30秒，那播放进度的记录频率就需要比较高。我们会在**前端每隔15秒就发起一次心跳请求,提交最新的播放进度，记录到服务端**。这样用户下一次续播时直接读取服务端的播放进度，就可以将时间误差控制在15秒左右。

## 2. 播放进度在服务端保存在哪里呢？是数据库吗？如果是数据库，如何解决高频写入给数据库带来巨大压力？
提交播放记录最终肯定是要保存到数据库中的。因为我们不仅要做视频续播，还有用户学习计划、学习进度统计等功能，都需要用到用户的播放记录数据。

但确实如你所说，前端每隔15秒一次请求，如果在用户量较大时，直接全部写入数据库，对数据库压力会比较大。因此我们采用了合并写请求的方案，当用户提交播放进度时会先缓存在Redis中，后续再将数据保存到数据库即可。

由于播放进度会不断覆盖，只保留最后一次即可。这样就可以大大减少对于数据库的访问次数和访问频率了。

## 3. Redis持久化时如何避免数据丢失或并发问题？

我们用Redisson的分布式锁来控制持久化任务，确保同时只有一个线程在操作数据库。Redis本身也支持AOF（追加日志文件）和RDB（快照备份），在Redis意外宕机时也能尽量减少数据丢失。

## 4. 如果定时任务失败导致数据未持久化，该如何补救？
- 使用幂等性设计：在任务执行时，每条数据都包含唯一标识（如课程ID、章节ID），即使任务重复执行，也不会影响数据库一致性。
- 增加重试机制：定时任务失败时，捕获异常并加入重试队列，通过延迟再执行。
- 数据持久化前，Redis中的进度设置过期时间避免占用内存。定时任务可通过持久化完成后删除Redis数据。
## 5. 如何处理Redis数据和数据库数据一致性问题？

## 6. 如何避免延迟任务丢失带来的播放进度错误？

## 7. 在超高并发场景下，Redis内存消耗如何优化？


# <font color="#245bdb">点赞系统:</font>
## 1. 能不能讲讲你们的点赞系统是如何设计的？

首先在设计之初我们分析了一下点赞业务可能需要的一些要求。

例如，在我们项目中需要用到点赞的业务不止一个，因此**点赞系统必须具备通用性，独立性，不能跟具体业务耦合。**

再比如，点赞业务可能会有较高的并发，我们要考虑到高并发写库的压力问题。

所以呢，我们在设计的时候，就**将点赞功能抽离出来作为独立服务**。当然这个服务中除了点赞功能以外，还有<u>与之关联的评价功能，不过这部分我就没有参与</u>了。**在数据层面也会用业务类型对不同点赞数据做隔离**。

从具体实现上来说，为了减少数据库压力，我们**会利用Redis来保存点赞记录、点赞数量信息**。然后**利用定时任务定期的将点赞数量同步给业务方，持久化到数据库中**。

## 2. 如果Redis中的点赞数据丢失了，会有什么影响？如何解决？

Redis数据丢失会影响短时间内的统计结果，但不会影响最终的数据一致性，因为定时任务会把Redis中的数据写到MySQL。如果担心丢失，我们可以缩短定时任务的间隔时间，同时启用Redis的AOF备份功能，定期恢复数据。


## 3. 那你们Redis中具体使用了哪种数据结构呢？

我们使用了两种数据结构，set和zset

首先保存点赞记录，使用了set结构，key是业务类型+业务id，值是点赞过的用户id。当用户点赞时就`SADD`用户id进去，当用户取消点赞时就`SREM`删除用户id。当判断是否点赞时使用`SISMEMBER`即可。当要统计点赞数量时，只需要`SCARD`就行，**而Redis的SET结构会在头信息中保存元素数量，因此SCARD直接读取该值，时间复杂度为O(1)，性能非常好。**

## 4. 为什么不用用户id为key，业务id为值呢?如果用户量很大，可能出现BigKey?

您说的这个方案也是可以的，不过呢，考虑到我们的**项目数据量并不会很大，我们不会有大V**，因此点赞数量通常不会超过1000，因此不会出现BigKey。并且，由于我们采用了业务id为KEY，当我们要统计点赞数量时，可以直接使用SCARD来获取元素数量，无需额外保存，这是一个很大的优势。但如果是考虑到有大V的场景，有两种选择，一种还是应该选择您说的这种方案，另一种则是**对用户id做hash分片，将大V的key拆分到多个KEY中，结构为 `[bizType:bizId:userId高8位]`.**

不过这里存在一个问题，就是页面需要判断当前用户有没有对某些业务点赞。这个时候会传来多个业务id的集合，而SISMEMBER只能一次判断一个业务的点赞状态，要判断多个业务的点赞状态，就必须多次调用SISMEMBER命令，与Redis多次交互，这显然是不合适的。（此处略停顿，等待面试官追问，面试官可能会问“那你们怎么解决的”。如果没追问，自己接着说），所以呢我们就**采用了Pipeline管道方式，这样就可以一次请求实现多个业务点赞状态的判断了**。

## 5. 那你ZSET干什么用的？

严格来说ZSET并不是用来实现点赞业务的，因为点赞只靠SET就能实现了。但是这里有一个问题，我们要**定期将业务方的点赞总数通过MQ同步给业务方，并持久化到数据库**。但是如果只有SET，我没办法知道哪些业务的点赞数发生了变化，需要同步到业务方。

因此，我们又添加了一个ZSET结构，用来记录点赞数变化的业务及对应的点赞总数。可以理解为一个待持久化的点赞任务队列。

每当业务被点赞，除了要缓存点赞记录，还要把业务id及点赞总数写入ZSET。这样定时任务开启时，只需要从ZSET中获取并移除数据，然后发送MQ给业务方，并持久化到数据库即可。

## 6. 那为什么一定要用ZSET结构，把更新过的业务扔到一个List中不行吗？

扔到List结构中虽然也能实现，但是存在一些问题：

首先，假设定时任务每隔2分钟执行一次，一个业务如果在2分钟内多次被点赞，那就会多次向List中添加同一个业务及对应的点赞总数，数据库也要持久化多次。这显然是多余的，因为只有最后一次才是有效的。而**使用ZSET则因为member的唯一性，多次添加会覆盖旧的点赞数量，最终也只会持久化一次**。

>（面试官可能说：“那就改为SET结构，SET中只放业务id，业务方收到MQ通知后再次查询不就行了。”如果没问就自己往下说）

当然要解决这个问题，也可以用SET结构代替List，然后当业务被点赞时，只存业务id到SET并通知业务方。<u>业务方接收到MQ通知后，根据id再次查询点赞总数从而避免多次更新的问题</u>。但是这种做法会导致多次网络通信，增加系统网络负担。而**ZSET则可以同时保存业务id及最新点赞数量，避免多次网络查询**。

不过，并不是说ZSET方案就是完全没问题的，**毕竟ZSET底层是哈希结构+跳表**，**对内存会有额外的占用**。但是考虑到我们的定时任务每次会查询并删除ZSET数据，ZSET中的数据量始终会维持在一个较低级别，内存占用也是可以接受的。

## 7. 在批量同步时如何防止缓存中的数据丢失？

## 8. 点赞状态如何处理分布式场景下的一致性问题？
# <font color="#245bdb">积分系统:</font>

## 1. 你项目中使用过Redis的那些数据结构啊？

**答:** 很多，比如String、Hash、Set、SortedSet、BitMap等

## 2. 能不能具体说说使用的场景?
**答:** 
比如很多的缓存，我们就使用了String结构来存储。还有点赞功能，我们用了Set结构和SortedSet结构。签到功能，我们用了BitMap结构。

就拿签到来说吧。因为签到数据量非常大嘛，而BitMap则是用bit位来表示签到数据，31bit位就能表示1个月的签到记录，非常节省空间，而且查询效率也比较高。

## 3. 你使用Redis保存签到记录，那如果Redis宕机怎么办?
**答:**
对于Redis的高可用数据安全问题，有很多种方案。

比如：我们可以给Redis添加**数据持久化机制,比如使用AOF持久化**。这样宕机后也丢失的数据量不多，可以接受。

或者呢，我们可以搭建**Redis主从集群,结合Redis哨兵**。主节点会把数据持续的同步给从节点，宕机后也会有哨兵重新选主，基本不用担心数据丢失问题。

当然，如果对于数据的安全性要求非常高。肯定还是要用传统数据库来实现的。但是为了解决签到数据量较大的问题，我们可能就需要**对数据做分表处理**了,或者及时将历史数据存档。

总的来说，签到数据使用Redis的BitMap无论是安全性还是数据内存占用情况，都是可以接受的。但是具体是选择Redis还是数据库方案，最终还是要看公司的要求来选择。

## 4. 你在项目中负责积分排行榜功能，说说看你们排行榜怎么设计实现的？
**答:**
我们的排行榜功能分为两部分：一个是当前赛季排行榜，一个是历史排行榜。
因为我们的产品设计是每个月为一个赛季，月初清零积分记录，这样学员就有持续的动力去学习。这就有了赛季的概念，因此也就有了当前赛季榜单和历史榜单的区分，其实现思路也不一样。
首先说当前赛季榜单，我们采用了Redis的SortedSet来实现。member是用户id，score就是当月积分总值。每当用户产生积分行为的时候，获取积分时，就会更新score值。这样Redis就会自动形成榜单了。非常方便且高效。
然后再说历史榜单，历史榜单肯定是保存到数据库了。不过由于数据过多，所以需要对数据做水平拆分，我们目前的思路是按照赛季来拆分，也就是每一个赛季的榜单单独一张表。这样做有几个好处：
- 拆分数据时比较自然，无需做额外处理
- 查询数据时往往都是按照赛季来查询，这样一次只需要查一张表，不存在跨表查询问题
因此我们就不需要用到分库分表的插件了，直接在业务层利用MybatisPlus就可以实现动态表名，动态插入了。简单高效。
我们会利用一个定时任务在每月初生成上赛季的榜单表，然后再用一个定时任务读取Redis中的上赛季榜单数据，持久化到数据库中。最后再有一个定时任务清理Redis中的历史数据。
这里要说明一下，这里三个任务是有关联的，之所以让任务分开定义，是为了避免任务耦合。这样在部分任务失败时，可以单独重试，无需所有任务从头重试。
当然，最终我们肯定要确保这三个任务的执行顺序，一定是依次执行的。

## 5. 你们使用Redis的SortedSet来保存榜单数据，如果用户量非常多怎么办？
**答:**
首先Redis的SortedSet底层利用了跳表机制，性能还是非常不错的。即便有百万级别的用户量，利用SortedSet也没什么问题，性能上也能得到保证。在我们的项目用户量下，完全足够。

当系统用户量规模达到数千万，乃至数亿时，我们可以采用分治的思想，将用户数据按照积分范围划分为多个桶。

然后为每个桶创建一个SortedSet类型的key，这样就可以将数据分散，减少单个KEY的数据规模了。

而要计算排名时，只需要按照范围查询出用户积分所在的桶，再累加分值范围比他高的桶的用户数量即可。依然非常简单、高效。

## 6. 你们使用历史榜单采用的定时任务框架是哪个？处理数百万的榜单数据时任务是如何分片的？你们是如何确保多个任务依次执行的呢？
**答:**
我们采用的是XXL-JOB框架。

XXL-JOB自带任务分片广播机制，每一个任务执行器都能通过API得到自己的分片编号、总分片数量。在做榜单数据批处理时，我们是按照分页查询的方式：

- 每个执行器的读取的起始页都是自己的分片编号+1，例如第一个执行器，其起始页就是1，第二个执行器，其起始页就是2，以此类推
    
- 然后不是逐页查询，而是有一个页的跨度，跨度值就是分片总数量。例如分了3片，那么跨度就是3
    

此时，第一个分片处理的数据就是第1、4、7、10、13等几页数据，第二个分片处理的就是第2、5、8、11、14等页的数据，第三个分片处理的就是第3、6、9、12、15等页的数据。

这样就能确保所有数据都会被处理，而且每一个执行器都执行的是不同的数据了。

  

最后，要确保多个任务的执行顺序，可以利用XXL-JOB中的子任务功能。比如有任务A、B、C，要按照字母顺序依次执行，我们就可以将C设置为B的子任务，再将B设置为A的子任务。然后给A设置一个触发器。

  

这样，当A触发时，就会依次执行这三个任务了。1




# <font color="#245bdb">优惠券系统</font>

# <font color="#245bdb">通用问题</font>

## 1. 请介绍天机学堂项目的整体架构设计？它是如何满足高并发和高性能需求的？

天机学堂采用了微服务架构，分成用户端和管理端两部分。每个模块，比如用户服务、课程服务、支付服务等，都是独立的微服务，用SpringCloud来实现。为了提高性能，我们使用了Redis和Redisson，分别负责缓存和分布式锁，减少了数据库压力。RabbitMQ用来做消息队列，实现异步解耦。任务调度方面用的是XXL-JOB，可以很好地满足我们定时任务的需求。  
高并发场景下，我们通过Redis的缓存机制和RabbitMQ的异步处理方式，把压力分散到多个服务节点，确保系统稳定运行。

## 2. 微服务架构有哪些典型问题？你们是如何解决的？

微服务常见问题有**服务间调用、配置管理、分布式事务**等。我们用Nacos做服务注册和配置管理，用OpenFeign简化服务调用，用Seata处理分布式事务。

## 3. 用户连点提交订单发送多个请求,你是如何避免重复生成多个订单的?

## 4. 微服务是如何获取用户信息的? JWT令牌的原理是什么?

## 5. 使用MyBatisPlus进行查询时,遇到`sum(xx),count(xx)`或实体类中不存在的字段,你是怎么进行存储的?
